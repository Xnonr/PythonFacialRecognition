{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs Dependencies\n",
    "#!pip install matplotlib\n",
    "#!pip install opencv-python\n",
    "#!pip install tensorflow\n",
    "#!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Dependencies\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# For generating unique image names, stands for Universally Unique Identifier (UUID)\n",
    "import uuid\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "# Imports model metric calculations\n",
    "from tensoflow.keras.metrics import Precision\n",
    "from tensoflow.keras.metrics import Precision\n",
    "\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384399bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sets GPU Growth\n",
    "\n",
    "Important in order to avoid potential Out Of Memory (OOM) erros as tensforlow by default will\n",
    "    take up as much memory as it can when running, therefore limiting GPU memory consumption\n",
    "    growth is rather important\n",
    "'''\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Folder Structures\n",
    "\n",
    "# Sets up data file paths\n",
    "data_file_path = '../../Data/FacialRecognition'\n",
    "\n",
    "POS_IMGS_PATH = os.path.join(data_file_path, 'Positive')\n",
    "NEG_IMGS_PATH = os.path.join(data_file_path, 'Negative')\n",
    "ANC_IMGS_PATH = os.path.join(data_file_path, 'Anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c026ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the actual files and directories\n",
    "os.makedirs(POS_IMGS_PATH)\n",
    "os.makedirs(NEG_IMGS_PATH)\n",
    "os.makedirs(ANC_IMGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01de0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects Positive & Negative Images\n",
    "\n",
    "# Untars Labelled Faces In The Wild Dataset\n",
    "\n",
    "# Decompresses the downloaded filed\n",
    "!tar -xf lfw.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5046d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moves all images to the negative images data repository and folder\n",
    "for directory in os.listdir('lfw'):\n",
    "    \n",
    "    # Use of the below if statement is due to MacOS file system structure\n",
    "    if directory != '.DS_Store':\n",
    "        for file in os.listdir(os.path.join('lfw', directory)):\n",
    "            EX_PATH = os.path.join('lfw', directory, file)\n",
    "            NEW_PATH = os.path.join(NEG_IMGS_PATH, file)\n",
    "            os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835fa942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects Positve & Anchor Classes\n",
    "\n",
    "# Establishes a connection to the computer's webcam\n",
    "image_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while image_capture.isOpened():\n",
    "    \n",
    "    # Returns the value as well as the actual captured image frame itself\n",
    "    return_value, image_frame = image_capture.read()\n",
    "    \n",
    "    '''\n",
    "    Slicing of the captured image frame to limit it to the desired 250 by 250 pixel dimensions\n",
    "        Specifying the range of values desired from the image capture\n",
    "    '''\n",
    "    image_frame = image_frame[120:120 + 250, 200:200 + 250, : ]\n",
    "    \n",
    "    # Collects an anchor image upon hitting the 'a' key upon the keyboard\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        \n",
    "        # Generates a unique file path\n",
    "        image_name = os.path.join(ANC_IMGS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        \n",
    "        # Writes out and saves the actual anchor image along with it's given generated name\n",
    "        cv2.imwrite(image_name, image_frame)\n",
    "    \n",
    "    # Collects a positive image upon hitting the 'p' key upon the keyboard\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        image_name = os.path.join(POS_IMGS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        \n",
    "        # Writes out and saves the actual anchor image along with it's given generated name\n",
    "        cv2.imwrite(image_name, image_frame)\n",
    "    \n",
    "    # Renders back and shows the captured image frame onto the screen\n",
    "    cv2.imshow('Image Collection: ', image_frame)\n",
    "    \n",
    "    # Breaking out of the loop gracefully, waiting for 1 milliseconds before waiting for the quitting key\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Releases the webcam\n",
    "image_capture.release()\n",
    "\n",
    "# Closes the frame displaying the captured image\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390be704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads & Preprocesses Images\n",
    "\n",
    "\n",
    "'''\n",
    "Obtains Image Directories\n",
    "\n",
    "Creates multiple segregated datasets from the previously established image directories\n",
    "Act as pipelines for all of the different images within the various directories\n",
    "Utilizes wild card searches for any files that end with the '.jpg' extension\n",
    "'''\n",
    "positive_dataset = tf.data.Dataset.list_files(POS_IMGS_PATH + '\\*.jpg').take(300)\n",
    "negative_dataset = tf.data.Dataset.list_files(NEG_IMGS_PATH + '\\*.jpg').take(300)\n",
    "anchor_dataset = tf.data.Dataset.list_files(ANC_IMGS_PATH + '\\*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scales & Resizes Images\n",
    "\n",
    "# Essentially returns the numpy equivalent of the image after processing\n",
    "def preprocess(file_path):\n",
    "    \n",
    "    # Reads in the image as a byte slice object from the given passed in file path\n",
    "    byte_image = tf.io.read_file(file_path)\n",
    "    \n",
    "    # Loads in the actual image\n",
    "    image = tf.io.decode_jpeg(byte_image)\n",
    "    \n",
    "    # Preprocessing step, resizing the image to fit 100 by 100 pixels with 3 color channels\n",
    "    image = tf.image.resize(image, (100, 100))\n",
    "    \n",
    "    # Scales the image values to be between 0 and 1\n",
    "    image = image / 255.0\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = preprocess('image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates A Labelled Dataset\n",
    "\n",
    "'''\n",
    "Zips up the datasets with a generated, equally long dataset of 1 values \n",
    "    in order to mark said images as positives and categorized, creating\n",
    "    a tuple of anchor and postive image file paths, along with the int\n",
    "    value labelling them\n",
    "'''\n",
    "labelled_positive_dataset = tf.data.Dataset.zip((anchor_dataset, \n",
    "                                                 positive_dataset, \n",
    "                                                 tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "\n",
    "'''\n",
    "Zips up the datasets with a generated, equally long dataset of 0 values \n",
    "    in order to mark said images as negatives and uncategorized\n",
    "'''\n",
    "labelled_negative_dataset = tf.data.Dataset.zip((anchor_dataset, \n",
    "                                                 negative_dataset, \n",
    "                                                 tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "\n",
    "labelled_dataset = labelled_positive_dataset.concatenate(labelled_negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6567fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = labelled_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e60fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_point = sample_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2658918",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32250fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Training & Testing Partitions\n",
    "\n",
    "# Preprocesses the dataset tuples' images\n",
    "def preprocess_twin(input_image, validation_image, label):\n",
    "    return(preprocess(input_image), preprocess(validation_image), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_preprocessing_result = preprocess_twin(*sample_data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample_preprocessing_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf411354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the data loader pipeline\n",
    "final_dataset = labelled_dataset.map(preprocess_twin)\n",
    "final_dataset = final_dataset.cache()\n",
    "final_dataset = final_dataset.shuffle(buffer_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4db0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_samples = final_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_dataset_samples.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bbc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishes the training partition\n",
    "training_dataset = final_dataset(round(len(final_dataset) * 0.7))\n",
    "training_dataset = training_dataset.batch(16)\n",
    "training_dataset = training_dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_samples = training_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad583bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_datapoint= training_dataset_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishes the testing partition\n",
    "testing_dataset = final_dataset.skip(round(len(final_dataset) * 0.7))\n",
    "testing_dataset = testing_dataset.take(round(len(final_dataset) * 0.3))\n",
    "testing_dataset = testing_dataset.batch(16)\n",
    "testing_dataset = testing_dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Engineering\n",
    "\n",
    "'''\n",
    "Builds the model embedding layer\n",
    "    Translating the input images of faces into an embedded layer feature vector\n",
    "    2 Rivers of data flowing through the neural network, the anchor and then positive or negative image\n",
    "        Forms the basis of the 1 shot classification outcome\n",
    "        Each river will output a feature vector of 4096 units\n",
    "'''\n",
    "def make_embedding_layer():\n",
    "    input1 = Input(shape = (100, 100, 3), name = 'input_image')\n",
    "    \n",
    "    # 1st Block\n",
    "    \n",
    "    # Passes 64 filters with a 10 by 10 pixel shape\n",
    "    convolution1 = Conv2D(64, (10, 10), activation = 'relu')(input1)\n",
    "    \n",
    "    # Condenses down the amount of data into a signle value within a 2 x 2 pixel area\n",
    "    max_pooling1 = MaxPooling2D(64, (2, 2), padding = 'same')(convolution1)\n",
    "    \n",
    "    # 2nd Block\n",
    "    convolution2 = Conv2D(128, (7, 7), activation = 'relu')(max_pooling1)\n",
    "    max_pooling2 = MaxPooling2D(64, (2, 2), padding = 'same')(convolution2)\n",
    "    \n",
    "    # 3rd Block\n",
    "    convolution3 = Conv2D(128, (4, 4), activation = 'relu')(max_pooling2)\n",
    "    max_pooling3 = MaxPooling2D(64, (2, 2), padding = 'same')(convolution3)\n",
    "    \n",
    "    # 4th Block\n",
    "    convolution4 = Conv2D(256, (4, 4), activation = 'relu')(max_pooling3)\n",
    "    \n",
    "    # Reduces everything down to a signle flat dimension\n",
    "    flatten1 = Flatten()(convolution4)\n",
    "    \n",
    "    dense1 = Dense(4096, activation = 'sigmoid')(flatten1)\n",
    "    \n",
    "    # Essentially returns a compiled version of the model\n",
    "    return Model(inputs = [input1], outputs = [dense1], name = 'embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_model = make_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19416ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29300633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds The Distance Layer\n",
    "\n",
    "'''\n",
    "Creates a custom neural network layer\n",
    "Have to combine and join the 2 rivers of data together, in this case by subtracting them from each other obtain\n",
    "    an L1 Siamese distance layer, telling us how similar the 2 images are allowing for image recognition\n",
    "Defining characteristc in a siamese neural network\n",
    "'''\n",
    "\n",
    "class L1Distance(Layer):\n",
    "    \n",
    "    '''\n",
    "    Base 'init' method within a python class, performs inheritance\n",
    "    'self' allows for taking actions upon itself\n",
    "    Inclusion of '**kwargs' in arguments allows for the usage of the class' abstracted method it inherits\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    '''\n",
    "    Core function tells the layer what actions to carry out when data is passed to it\n",
    "        The 1st data river representing the anchor image is the 'input embedding'\n",
    "        The 2nd data river representing the positive or negative image is the 'validation embedding'\n",
    "        Returns the absolute value difference between the two embedding layers created from the \n",
    "            original images\n",
    "    Performs a similarity calculation\n",
    "    '''\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_distance_layer = L1Distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds A Siamese Neural Network Model\n",
    "def make_siamese_model():\n",
    "    \n",
    "    # Handles inputs, being 2 data streams, matches the shapes of the input images, raw\n",
    "    \n",
    "    # Anchor image input within the network\n",
    "    input_image = Input(name = 'input_image', shape = (100, 100, 3))\n",
    "    \n",
    "    # Validation image input within the network\n",
    "    validation_image = Input(name = 'validation_image', shape = (100, 100, 3))\n",
    "    \n",
    "    # Combines the Siamese distance components\n",
    "    siamese_layer = L1Distance()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding_layer_model(input_image), embedding_layer_model(validation_image))\n",
    "    \n",
    "    '''\n",
    "    Classification layer\n",
    "        Passing in 4096 units in\n",
    "        Output 1 unit out, being either a value of 1 or 0 due to the sigmoid activation\n",
    "        Classifies as either a match or a non match\n",
    "    '''\n",
    "    classifier = Dense(1, activation = 'sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs = [input_image, validation_image], outputs = classifier, name = 'SiameseNeuralNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc955c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_neural_network = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a756aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_neural_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Neural Network Model Training\n",
    "\n",
    "# Sets Up Loss & Optimizer\n",
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b57062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishes Checkpoint Callbacks\n",
    "checkpoint_file_path_directory = '../../Data/Models/Model_Training_Checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_file_path_directory, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer = optimizer, siamese_neural_network = siamese_neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd04b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds Training Step Function\n",
    "\n",
    "'''\n",
    "What is used to effectively train upon 1 batch of data\n",
    "    Makes a prediction\n",
    "    Calculates loss\n",
    "    Calculates gradients\n",
    "    Apply backpropagation throughout the neural network in order to obtain the best possible model\n",
    "Sames steps with whatever sort of neural network\n",
    "'''\n",
    "\n",
    "# Wraps the function with the decorator in order to compile said function into a callable TensorFlow graph\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Allows for the capturing of gradient values produced by the neural network model\n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "        # Retrives the features, the anchor and positive / negative images\n",
    "        x = batch[:2]\n",
    "        \n",
    "        # Retrives the label\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Passes data into the siamese model for prediction making\n",
    "        y_hat = siamese_neural_network(x, training = True)\n",
    "        \n",
    "        # Calculates the training loss\n",
    "        loss = binary_cross_loss(y, y_hat)\n",
    "        \n",
    "    # Calculates gradients for the loss with respect to these trainable variables of the given model\n",
    "    gradient = tape.gradient(loss, siamese_neural_network.trainable_variables)\n",
    "    \n",
    "    # Calculates updated weights and applies them to the model via backpropagation\n",
    "    optimizer.apply_gradients(zip, gradient, siamese_neural_network.trainnable_variables)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28dcf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds The Training Loop\n",
    "\n",
    "# Iterates and trains the siamese model over every batch made available from the dataset\n",
    "def train(data, EPOCHS):\n",
    "    \n",
    "    # Loops through the epochs\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f'\\n Epoch {epoch}/{EPOCHS}')\n",
    "        progress_bar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Loops through each batch\n",
    "        for index, batch in enumerate(data):\n",
    "            train_step(batch)\n",
    "            progress_bar.update(index + 1)\n",
    "            \n",
    "        # Optionally saves checkpoints after every 10 epochs of training\n",
    "        #if epoch % 10 == 0\n",
    "            #checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35362201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains The Model\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfcbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(training_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates The Model\n",
    "\n",
    "# Retrives a single batch of testing data as a numpy equivalent\n",
    "test_input, test_validation, y_true = testing_dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carries out predictions\n",
    "y_hat = siamese_neural_network.predict([test_input, test_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing result for easier interpretability\n",
    "[1 if prediction > 0.5 else 0 for prediction in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c097ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a metric object specifically for recall and precision\n",
    "metric1 = Recall()\n",
    "metric2 = Precision()\n",
    "\n",
    "# Calculates the recall and precision values, updating over time\n",
    "metric1.update_state(y_true, y_hat)\n",
    "metric2.update_state(y_true, y_hat)\n",
    "\n",
    "# Returns the results\n",
    "print(metric1.result().numpy())\n",
    "print(metric2.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizes Results\n",
    "\n",
    "# Sets the overall plot's size\n",
    "plt.figure(figsize = (18, 8))\n",
    "\n",
    "# Sets the 1st subplot, with the number of row, column and its index within\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_input[0])\n",
    "\n",
    "# Sets the 2nd subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_validation[0])\n",
    "\n",
    "# Renders both the test and validation images cleanly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2363f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves The Model\n",
    "\n",
    "model_weights_file_path = '../../Data/Models'\n",
    "\n",
    "siamese_neural_network.save(os.path.join(model_weights_file_path, 'siamese_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads The Model\n",
    "model = tf.keras.models.load_model(os.path.join(model_weights_file_path, 'siamese_model.h5'),\n",
    "                                   custom_objects = {'L1Distance':L1Distance, \n",
    "                                                     'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Time Verification\n",
    "\n",
    "# Verification Function\n",
    "def verify(detection_threshold, verification_threshold):\n",
    "    \n",
    "    # Instantiates a results array\n",
    "    results = []\n",
    "    \n",
    "    '''\n",
    "    Loops through all available positive images as validation against the single image being compared\n",
    "        in order to maximize the chance of a correct prediction being made when given a live feed\n",
    "    '''\n",
    "    for image in os.listdir(POS_IMGS_PATH):\n",
    "        input_image = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "        validation_image = preprocess(os.path.join(POS_IMGS_PATH, image))\n",
    "        \n",
    "        # Makes a prediction and appends it to the results\n",
    "        result = model.predict(list(np.expand_dims([input_image, validation_image], axis = 1)))\n",
    "        results.append(result)\n",
    "    \n",
    "    # Detection Threshold: Metric above which a prediction is considered positive\n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    \n",
    "    # Verification Threshold: Proportion of positive predictions over total positive samples\n",
    "    verification = detection / len(os.listdir(POS_IMGS_PATH))\n",
    "    verified = verificiation > verification_threshold\n",
    "    \n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV Real Time Verification\n",
    "\n",
    "image_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while image_capture.isOpened():\n",
    "    return_value, image_frame = image_capture.read()\n",
    "\n",
    "    image_frame = image_frame[120:120 + 250, 200:200 + 250, : ]\n",
    "\n",
    "    cv2.imshow('Verification: ', image_frame)\n",
    "    \n",
    "    # Verification trigger\n",
    "    if cv2.waitKey(10) & 0XFF == ord('v'):\n",
    "        \n",
    "        # Saves the input image to the applicatio data folder\n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), image_frame)\n",
    "        \n",
    "        # Runs verification\n",
    "        results, verfied = verify(model, 0.9, 0.7)\n",
    "        print(verfied)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "image_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5cb8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052d696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
